{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3db42e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "import gc\n",
    "from numba import cuda\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfcf817",
   "metadata": {},
   "source": [
    "#### Define path to train data file and validation data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "503d3735",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r\"data/train.jsonl\"\n",
    "validation_path = r\"data/validation.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745436b6",
   "metadata": {},
   "source": [
    "#### Load data and encoding class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68689a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file, relevant_cols):\n",
    "    '''\n",
    "    Load data from file, retrieve only the relevant columns into a Pandas DataFrame.\n",
    "    Concatentate all text datas from relevent columns in to a \"text\" column in Data-\n",
    "    Frame.\n",
    "    \n",
    "    Args:\n",
    "        file: path to the data file\n",
    "        relevant_cols: relevant columns (features) to be used\n",
    "    \n",
    "    Returns:\n",
    "        df: a Pandas DataFrame containing the relevant data\n",
    "    '''\n",
    "    df = pd.read_json(file, lines=True)\n",
    "    df[\"text\"] = df[\"postText\"].explode() + df[\"targetTitle\"].explode()\n",
    "    df = df.loc[:, relevant_cols]\n",
    "    df.tags = df.tags.explode()\n",
    "    df.tags = df.tags.apply(label_encoding, args=(\"\", ))\n",
    "    return df\n",
    "\n",
    "\n",
    "def label_encoding(text, target=\"\"):\n",
    "    '''\n",
    "    Encode class label from String to Integer\n",
    "    \n",
    "    Args:\n",
    "        text: label that needs to be encoded\n",
    "        target:\n",
    "            default: None. 3 Labels will be encoded to 1,2, and 3 respectively.\n",
    "            If one label is given, this label will be encoded as 1. The other \n",
    "            twos labels will be encoded as 0.\n",
    "            \n",
    "    Returns:\n",
    "        Encoding according to label given.\n",
    "    '''\n",
    "    if target:\n",
    "        if text == target:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if text == \"phrase\":\n",
    "            return 0\n",
    "        elif text == \"passage\":\n",
    "            return 1\n",
    "        elif text == \"multi\":\n",
    "            return 2\n",
    "        else:\n",
    "            return text    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ca8c0",
   "metadata": {},
   "source": [
    "#### Load Train Data\n",
    "    We will use postText and targetTitle texts for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8e6f6d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wes Welker Wanted Dinner With Tom Brady, But P...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NASA sets date for full recovery of ozone hole...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is what makes employees happy -- and it's...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Passion is overrated — 7 work habits you need ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The perfect way to cook rice so that it's perf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>Has Facebook's video explosion completely shak...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>Cop Is Eating At A Chili's When Teen Hands Him...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>5 popular myths about visible signs of aging t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>You need to see this Twitter account that pred...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>GOP congressman comes out for gay marriagePenn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  tags\n",
       "0     Wes Welker Wanted Dinner With Tom Brady, But P...     1\n",
       "1     NASA sets date for full recovery of ozone hole...     0\n",
       "2     This is what makes employees happy -- and it's...     0\n",
       "3     Passion is overrated — 7 work habits you need ...     2\n",
       "4     The perfect way to cook rice so that it's perf...     0\n",
       "...                                                 ...   ...\n",
       "3195  Has Facebook's video explosion completely shak...     1\n",
       "3196  Cop Is Eating At A Chili's When Teen Hands Him...     1\n",
       "3197  5 popular myths about visible signs of aging t...     2\n",
       "3198  You need to see this Twitter account that pred...     0\n",
       "3199  GOP congressman comes out for gay marriagePenn...     0\n",
       "\n",
       "[3200 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_columns = [\"text\", \"tags\"]\n",
    "train_set = load_data(train_path, relevant_columns)\n",
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9473cf",
   "metadata": {},
   "source": [
    "#### Load Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17aae3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Five Nights at Freddy’s Sequel Delayed for Wei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why Arizona Sheriff Joe Arpaio’s fate could ha...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Here’s how much you should be tipping your hai...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Harry Potter\" alums reunite for new movieAlan...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A man swallowed a microSD card and you won't b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>This is what happens when you leave a hotel cl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>This Texas GOP elector announces that he won't...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>This beauty queen cured her acne with one diet...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>WikiLeaks' Julian Assange Reported DeadWikiLea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>Original \"Law &amp;amp; Order: SVU\" cast member le...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  tags\n",
       "0    Five Nights at Freddy’s Sequel Delayed for Wei...     1\n",
       "1    Why Arizona Sheriff Joe Arpaio’s fate could ha...     2\n",
       "2    Here’s how much you should be tipping your hai...     0\n",
       "3    \"Harry Potter\" alums reunite for new movieAlan...     2\n",
       "4    A man swallowed a microSD card and you won't b...     1\n",
       "..                                                 ...   ...\n",
       "795  This is what happens when you leave a hotel cl...     1\n",
       "796  This Texas GOP elector announces that he won't...     0\n",
       "797  This beauty queen cured her acne with one diet...     2\n",
       "798  WikiLeaks' Julian Assange Reported DeadWikiLea...     1\n",
       "799  Original \"Law &amp; Order: SVU\" cast member le...     0\n",
       "\n",
       "[800 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set = load_data(validation_path, relevant_columns)\n",
    "validation_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73423a82",
   "metadata": {},
   "source": [
    "#### Set up configurations for training transformer model\n",
    "    - Because of time constraints, two hyper parameters selected in validation is: learning rate and warm up ratio\n",
    "    - Warm-up is a way to reduce the primacy effect of the early training examples. The learning rate is increased linearly over the warm-up period. This often leads to faster training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67f9c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations = []\n",
    "\n",
    "for learning_rate in [4e-6, 1e-5, 4e-5]:\n",
    "    for warumup_ratio in [0.02, 0.06, 0.1]:\n",
    "        configurations += [{\n",
    "            \"overwrite_output_dir\": True,\n",
    "            \"num_train_epochs\": 10,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"warmup_ratio\": warumup_ratio,\n",
    "            \"best_model_dir\": r\"outputs/distilbert/best_model\",\n",
    "            \"output_dir\" : \"outputs/\"\n",
    "        }]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa92cde",
   "metadata": {},
   "source": [
    "#### Train model with the above given configurations.\n",
    "    - The model being train is a distilled version of BERT, aply named distilBERT, which is smaller and faster #to train but at the cost of accuracy.\n",
    "    - num_labels set to 3 for fine-tuning with our dataset.\n",
    "    - ignore_mismatched_sizes=True is needed if the model_type and the model itself have some discrepancies.\n",
    "    - delete model and flush CUDA memory after training each model to preven out of memory error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78f1e022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e53235e1aa404d899a686d69a61ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\phanm\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb6b756e23347c8a69d867a1f22c816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/263M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing BertForSequenceClassification: ['distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['encoder.layer.3.intermediate.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'pooler.dense.bias', 'classifier.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'classifier.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b852d8a3c448fca084840386dbbd19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81ed9b94abf4b259401709051bce667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd163bd47d7a46e7ba2f34cfec3c2ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n",
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:612: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592acd5c8221439ea527add675f05c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d6cf53b6a94bd8a07e8ea9bf2d5824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675f789bd49c416dbdc4e6f741626abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13dcb854d29d4d2cbbb22265ace849f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5f7b71ad7940dbbb250792248ad195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522ff4c67884422a9f59460cd848f005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d94e099e3440ce8bc8f013ff5e5ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 4 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db69bf603d4491dbc8ddde2e3a0b510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 5 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1911b68fe2c0430597dc476c218af280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 6 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50e9cff590b453481d45dcd52edb8a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 7 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febb5953799d4e6fbe7e7a5f7f981116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 8 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ce336665774a7db6cedfb6988e0af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 9 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e268bf0be9884f3eab8f52a74db07839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05bdc05de7f647d2a8b8cb476458c0eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe943899b0c540b3b02805c09927d78e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed959fbe597f44fb8f551eab34919c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing BertForSequenceClassification: ['distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['encoder.layer.3.intermediate.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'pooler.dense.bias', 'classifier.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'classifier.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n",
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:612: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36909fa8bcbe4ef6ae583d91bacbebcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59e210d3e134394bd9b822ddb0bf4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3d50f469474e6482e98389b386c03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8959abab86d04683846f98bef8c10613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff110f1af4d4e77a6a69877e03b7286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0e419e665b489c92059f21219c30ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aabe6ae4ad2d46b8a2b365999c7c9c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 4 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8dd95730ee84c16aa159223e3cfc5fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 5 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97905682527840e09cb7135b4675db4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 6 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807b98ecfcd44d79bfd5acb3dbb562c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 7 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a8a0f66ae141ab88db90f9af6b37f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 8 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0485b74905904fbbbdb43ef8cc273aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 9 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ff121bfb4949afae63ae43af26a974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ad726644eb4d6099cd48522834d8e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46586362db47496690d0528d7914b9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e25bf174cd449cb2312c87b78274d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing BertForSequenceClassification: ['distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['encoder.layer.3.intermediate.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'pooler.dense.bias', 'classifier.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'classifier.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n",
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:612: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb042303d2a4dfe8d10a03272393ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109fb9d817214038b3bd091087bf7eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f110277263456ab0d24266db620057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60aa5516208f45a184eb345166b807e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce4f9aea68140fb894fe33bcc2ee56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9bcbf7d774f46a2afd2a9ca984f533a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f535132b6a004925b041d3986de9ee52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 4 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68aecb0a66af41518426ad38c46c4e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 5 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2609597a8e0346be8896e994de1b1631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 6 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b48b798b46847a8ac01d48f888d725d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 7 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b83d9f000ac45cab1ac2ea3a339ceca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 8 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34b6dcca8344f2bbb060f50db333184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 9 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34cb73f6078f49b0bb2cf1b6330971dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1b5ddb4f564e1c888a037955ab9f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0327dd414d5439a9f73984a50791d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa18a88918e4c7db85da5527c76b236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing BertForSequenceClassification: ['distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['encoder.layer.3.intermediate.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'pooler.dense.bias', 'classifier.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'classifier.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n",
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:612: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac9deaf46124666bba75b074c877198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df4df381b9b4991a8fbf27a1ab650dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a8538f5cf14e2ba40b50936fdd6c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf78bf1a40394400b7c026e0c0cadff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b22585bf6546f39c52587edc8b4d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a70262f21eb04868aecdb6307b9eaaad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b5be2eb9a74eb19ed89af7f854f1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 4 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38025160b48044f3b0f8c39c536f02f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 5 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de089f8c5924025b843103023dc1a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 6 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62371f4b77244419be014c0dfbfb8cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 7 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78abb0be6e2e40d49f794975601b84a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 8 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60c9045344149949d6ce66052890dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 9 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5918649f0a84cb5a6c50d6c9f6aa201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4593b9ecfb6a47ecae8360029fcb4f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e76205496444308bb6ff6c7615e4a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d5495937a44f78b4ec0d6d0822ed0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing BertForSequenceClassification: ['distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['encoder.layer.3.intermediate.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'pooler.dense.bias', 'classifier.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'classifier.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n",
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:612: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7700b45730a44fb293a681aedca0ba7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a653e0b8aa448bc8f7bc208ed19d4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8747227d9a064400b69a1c9d1c05be0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb54503c80eb4239b0cb44bf303c7d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2852d4d9df4b42c39ec311ba29caee7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb332b1f56d1446592fc10cdbf7bc1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12845abf94c4928a410e6b6d6ff8eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 4 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98d7afc820142c8a4a5bf221339bcc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 5 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e58d6463e646a6870733585efd414b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 6 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbdc7895dac14d63995bbf7ef4ddd241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 7 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb9fffd9cbf4e7381d281b1e87e0e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 8 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79ac48f0b5f45b09dafe7842dab702b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 9 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6365775324cc4a4aa8b33c74d3b66aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5efd9d87b914ad980221f85df747bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a20379fb2704243a08de03bd5ed9437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690714db993149c89744893eccf36c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing BertForSequenceClassification: ['distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['encoder.layer.3.intermediate.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'pooler.dense.bias', 'classifier.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'classifier.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n",
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:612: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca45ed32c20246b4a34ba9e401acc932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee37706542a749e8aba1e877d66ad383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbc164013ec439cb46eea7a5de9ca1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de04f91abc4f4a4490d8b26b6c3c8d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d43ac0962df425c9b7f532745815d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812b43137f7e42fcba720e9090207c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0230164fdea14f138e12ea8090a0b38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 4 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154bc04467aa4078afe673803d53f451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 5 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101211649d18485d84baedf12909860b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 6 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a8960435fa428dae67dbb6176c50fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 7 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe346481f5f43b786baf58eb1813f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 8 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b382729e5445c6868693fd0de90933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 9 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5e85d1b9a845e4935b6ec13991c391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5802a14f4e7f4db1b40c87e43032470f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52bbc3e3c4884e47b147d60c6d1492f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62d720bdd1a4806b99ece49dfa38e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing BertForSequenceClassification: ['distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['encoder.layer.3.intermediate.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'pooler.dense.bias', 'classifier.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'classifier.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n",
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:612: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb0667cc8a9442c8145f1cdd16fba04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0fe0ffb63245c48680688a5b2d65a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c5a9e637124290a8befa5550fb7fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495860fb50894e0cb4b2b07ee39ef376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf188808543248deb650346b15ee445f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7cef6551ff4c499482a0dd03fa96d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb1c443101a54697ab5f871c762f1784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 4 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d1052d67934a8ab5597fcdb761d055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 5 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe443a844c648f2a9c1394165a637db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 6 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291ebd81f51e4e7d95a4278b2515c371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 7 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0190f86cad74635a145aa4bbfcf883f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 8 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96bb9b8ac325411ea9bb167a1ba2b3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 9 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987cc42266a544abbd28dd89fb5c1e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc03c9789f746d6bf3916f9b116fd5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82494ab9e8494751a9068baf2f988186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2877387c85b14be4b34b8e99326d3de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing BertForSequenceClassification: ['distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['encoder.layer.3.intermediate.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'pooler.dense.bias', 'classifier.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'classifier.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n",
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:612: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8bd006be964ca7903c849879c511a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b023f8f7536641f4998e56a4414b6aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789ea4a2be45417aa9aa78344a33b21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f67606e8d9e4e92a493b1f47fa8674b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf75cfa477e47fdad70bc0c2a8e0555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec60c6aeaafd49b2a80e33b7bb491c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0366750baf46aaa9d957ef6d22001a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 4 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77bd85c4392b457da75e8aa2fcf69be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 5 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8833c1a71d19466aa3339e9a56f92159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 6 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3341a639f648959c184eb4d5e59adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 7 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1303103877b844f0bf468d942177e903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 8 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cec8174bfa440b8a446bc5a857f502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 9 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e5ad90ef9834a2a986e1067e56b13c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930ad35efaec4c1abc8594cf8f439900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8771ea11c40b41bd8f946a2c4908ff16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f1c4628a32432382ae671627cdf5c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing BertForSequenceClassification: ['distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['encoder.layer.3.intermediate.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'pooler.dense.bias', 'classifier.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'classifier.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n",
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:612: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023b4a48cbc447d4a6df15ab659e4c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6517378f0014570940c75b4c4916345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435e4e29e77a4b9c8d7cc37f7631fd35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "decb375a1ae347008a67a975454cd8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4beeaac7a03f414380be338182fda74f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27794995e1f4424846b2cd37116a9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db6f2d2f9964ecdb494bade32a54cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 4 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9481d03ca62492aaadab0a0b000ced0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 5 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803744f3ae2743b4a7b2c0972c103231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 6 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7fd38cecf6428d8d6ef6534a8c79e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 7 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0fb6d6e9bb841c1ba7fcd3ece474360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 8 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08feb95d59b2416e85c313f81e9c4b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 9 of 10:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79cc1665172f46eb8f8861e23ed32d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417a482927dc4686b23b5a9c88de53e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phanm\\miniconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e239bbe0ba4c61a86ed07bd2182ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca1f332f69346c9ae5c3cb18354311e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for config in configurations:\n",
    "    \n",
    "    config[\"output_dir\"] = r\"outputs/distilbert/distilbert_\" + str(config[\"learning_rate\"]) + \"_\" + str(config[\"warmup_ratio\"])\n",
    "        \n",
    "    model = ClassificationModel(\"bert\", \"distilbert-base-cased\", num_labels=3, args=config, ignore_mismatched_sizes=True)\n",
    "    model.train_model(train_set, eval_df=validation_set, acc=accuracy_score)\n",
    "    train = model.eval_model(train_set, acc=accuracy_score)\n",
    "    evaluation = model.eval_model(validation_set, acc=accuracy_score)\n",
    "    \n",
    "    results += [(config[\"learning_rate\"], config[\"warmup_ratio\"], train[0], evaluation[0])]\n",
    "    model = None\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5bcb70",
   "metadata": {},
   "source": [
    "#### Model Selection using Validation Set\n",
    "    - The best model is selected using accuracy as validation metric.\n",
    "    - All model results are then saved to a dataframe and a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5b2c202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Warmup Ratio</th>\n",
       "      <th>acc</th>\n",
       "      <th>eval_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Distilbert 1e-05_0.1</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.4178257846832276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Distilbert 1e-05_0.06</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.5725</td>\n",
       "      <td>1.3830605363845825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Distilbert 4e-05_0.1</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.565</td>\n",
       "      <td>1.625411958694458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Distilbert 4e-06_0.06</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.55375</td>\n",
       "      <td>1.060385627746582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Distilbert 1e-05_0.02</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.5525</td>\n",
       "      <td>1.3568671321868897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Distilbert 4e-06_0.02</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.54375</td>\n",
       "      <td>1.0574987411499024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Distilbert 4e-06_0.1</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>1.1028425216674804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Distilbert 4e-05_0.02</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.41875</td>\n",
       "      <td>1.040203857421875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Distilbert 4e-05_0.06</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.41875</td>\n",
       "      <td>1.0390802001953126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model name  Learning rate  Warmup Ratio      acc  \\\n",
       "5   Distilbert 1e-05_0.1       0.000010          0.10     0.58   \n",
       "4  Distilbert 1e-05_0.06       0.000010          0.06   0.5725   \n",
       "8   Distilbert 4e-05_0.1       0.000040          0.10    0.565   \n",
       "1  Distilbert 4e-06_0.06       0.000004          0.06  0.55375   \n",
       "3  Distilbert 1e-05_0.02       0.000010          0.02   0.5525   \n",
       "0  Distilbert 4e-06_0.02       0.000004          0.02  0.54375   \n",
       "2   Distilbert 4e-06_0.1       0.000004          0.10   0.5375   \n",
       "6  Distilbert 4e-05_0.02       0.000040          0.02  0.41875   \n",
       "7  Distilbert 4e-05_0.06       0.000040          0.06  0.41875   \n",
       "\n",
       "            eval_loss  \n",
       "5  1.4178257846832276  \n",
       "4  1.3830605363845825  \n",
       "8   1.625411958694458  \n",
       "1   1.060385627746582  \n",
       "3  1.3568671321868897  \n",
       "0  1.0574987411499024  \n",
       "2  1.1028425216674804  \n",
       "6   1.040203857421875  \n",
       "7  1.0390802001953126  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict = {\"Model name\" : [],\n",
    "                \"Learning rate\" : [],\n",
    "                \"Warmup Ratio\" : [],\n",
    "                \"acc\" : [],\n",
    "                \"eval_loss\" : []\n",
    "               }\n",
    "for config in configurations:\n",
    "    model_name = str(config[\"learning_rate\"]) + \"_\" + str(config[\"warmup_ratio\"])\n",
    "    with open(r\"outputs/distilbert/distilbert_\" + model_name + r\"/eval_results.txt\", \"r+\") as file:\n",
    "        data = file.readlines()\n",
    "        results_dict[\"Model name\"].append(\"Distilbert \" + model_name)\n",
    "        results_dict[\"Learning rate\"].append(config[\"learning_rate\"])\n",
    "        results_dict[\"Warmup Ratio\"].append(config[\"warmup_ratio\"])\n",
    "        results_dict[\"acc\"].append(data[0][6:-1])\n",
    "        results_dict[\"eval_loss\"].append(data[1][12:-1])\n",
    "df = pd.DataFrame.from_dict(results_dict).sort_values(\"acc\", ascending=False)\n",
    "df.to_csv(\"distilbert_validation_results.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
