{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3db42e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "import gc\n",
    "from numba import cuda\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165a4880",
   "metadata": {},
   "source": [
    "#### Load data and encoding class labels\n",
    "    - Can encode as multi-class or binary training/validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68689a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file, relevant_cols, one_class=\"\"):\n",
    "    '''\n",
    "    Load data from file, retrieve only the relevant columns into a Pandas DataFrame.\n",
    "    Concatentate all text datas from relevent columns in to a \"text\" column in Data-\n",
    "    Frame.\n",
    "    \n",
    "    Args:\n",
    "        file: Path to the data file\n",
    "        relevant_cols: Relevant columns (features) to be used\n",
    "        one_class:\n",
    "            default:- None. 3 labels will be encoded to 0,1, and 2 respectively.\n",
    "                    - If a label is given, convert the data to binary-labeled,\n",
    "                      with the given class is \"1\", the rest is \"0\".\n",
    "    Returns:\n",
    "        df: a Pandas DataFrame containing the relevant data\n",
    "    '''\n",
    "    df = pd.read_json(file, lines=True)\n",
    "    df[\"text\"] = \"\"\n",
    "    for col in relevant_cols:\n",
    "        df[\"text\"] += df[col].explode() + \". \"\n",
    "    df = df.loc[:, [\"text\", \"tags\"]]\n",
    "    df.tags = df.tags.explode()\n",
    "    df.tags = df.tags.apply(label_encoding, args=(one_class, ))\n",
    "    return df\n",
    "\n",
    "\n",
    "def label_encoding(text, one_class=\"\"):\n",
    "    '''\n",
    "    Encode a class label from String to Integer\n",
    "    \n",
    "    Args:\n",
    "        text: Label that needs to be encoded\n",
    "        one_class:\n",
    "            default:- None. 3 labels will be encoded to 1,2, and 3 respectively.\n",
    "                    - If a label is given, convert the data to binary-labeled,\n",
    "                      with the given class is \"1\", the rest is \"0\".\n",
    "            \n",
    "    Returns:\n",
    "        Encoding according to label given.\n",
    "    '''\n",
    "    if one_class:\n",
    "        if text == one_class:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if text == \"phrase\":\n",
    "            return 0\n",
    "        elif text == \"passage\":\n",
    "            return 1\n",
    "        elif text == \"multi\":\n",
    "            return 2\n",
    "        else:\n",
    "            return text    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27731178",
   "metadata": {},
   "source": [
    "#### Set up configurations for training transformer model\n",
    "    - Because of time constraints, two hyper parameters selected in validation is: learning rate and warm up ratio\n",
    "    - Warm-up is a way to reduce the primacy effect of the early training examples. The learning rate is increased linearly over the warm-up period. This often leads to faster training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67f9c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_config(model_type, learn_rate, warmup, epochs, batch_size):\n",
    "    '''\n",
    "    Make a list of configurations given a list of learning rate and warmup ratio\n",
    "    \n",
    "    Args:\n",
    "        learn_rate: a list of learning rate\n",
    "        warmup: a list of warmup ratio\n",
    "        \n",
    "    Returns:\n",
    "        configurations: a list of configurations\n",
    "    '''\n",
    "    configurations = []\n",
    "\n",
    "    for learning_rate in learn_rate:\n",
    "        for warmup_ratio in warmup:\n",
    "            for epoch in epochs:\n",
    "                for batch in batch_size:\n",
    "                    configurations += [{\"overwrite_output_dir\": True,\n",
    "                                        \"num_train_epochs\": epoch,\n",
    "                                        \"learning_rate\": learning_rate,\n",
    "                                        \"warmup_ratio\": warmup_ratio,\n",
    "                                        \"train_batch_size\" : batch,\n",
    "                                        \"best_model_dir\": fr\"outputs/{model_type}/best_model\",\n",
    "                                        \"output_dir\" : \"outputs/\"\n",
    "                                        }]\n",
    "    return configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e82b70b",
   "metadata": {},
   "source": [
    "#### Train model with a given set of configurations.\n",
    "    - The model being train is a base version of deberta, downloaded from Hugging Face using the simpletransformers library.\n",
    "    - num_labels set to 3 for fine-tuning with our dataset.\n",
    "    - ignore_mismatched_sizes=True is needed if the model_type and the model itself have some discrepancies.\n",
    "    - delete model and flush CUDA memory after training each model to preven out of memory error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f1e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_set, validation_set, model_type, model_name, configurations, num_labels=3):\n",
    "    '''\n",
    "    Train the model for different configurations\n",
    "    \n",
    "    Args:\n",
    "        train_set: training data in DataFrame format\n",
    "        validation_set: validation data in DataFrame format\n",
    "        model_type: type of model to train\n",
    "        model_name: name of the model on Hugging Face\n",
    "        configurations: a list of configurations to train the model on\n",
    "        \n",
    "    Returns:\n",
    "        results: List containing the evaluation accuracy results\n",
    "    '''\n",
    "    results = []\n",
    "\n",
    "    for config in configurations:\n",
    "\n",
    "        config[\"output_dir\"] = fr\"outputs/{model_type}/{model_type}_\" + \\\n",
    "                                str(config[\"learning_rate\"]) + \"_\" + \\\n",
    "                                str(config[\"warmup_ratio\"]) + \"_e\" + \\\n",
    "                                str(config[\"num_train_epochs\"]) + \"_b\" + \\\n",
    "                                str(config[\"train_batch_size\"])\n",
    "        model = ClassificationModel(model_type, model_name, num_labels=num_labels, args=config, ignore_mismatched_sizes=True)\n",
    "        model.train_model(train_set, eval_df=validation_set, acc=accuracy_score)\n",
    "        train = model.eval_model(train_set, acc=accuracy_score)\n",
    "        evaluation = model.eval_model(validation_set, acc=accuracy_score)\n",
    "\n",
    "        results += [(config[\"learning_rate\"], config[\"warmup_ratio\"], train[0], evaluation[0])]\n",
    "        model = None\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b656d6",
   "metadata": {},
   "source": [
    "#### Model Selection using Validation Set\n",
    "    - The best model is selected using accuracy as validation metric.\n",
    "    - All model results are then saved to a dataframe and a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9166e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_table(model_type, configurations):\n",
    "    '''\n",
    "    Retrieve results from eval_results.txt in outputs/{model_name}\n",
    "    \n",
    "    Args:\n",
    "        model_type: model type that was trained\n",
    "        configurations: configurations that were used to trained the model\n",
    "        \n",
    "    Returns:\n",
    "        df: DataFrame containing the accuracy metric results\n",
    "    '''\n",
    "    results_dict = {\"model_name\" : [],\n",
    "                    \"learning_rate\" : [],\n",
    "                    \"warmup_ratio\" : [],\n",
    "                    \"num_epochs\" : [],\n",
    "                    \"batch_size\" : [],\n",
    "                    \"acc\" : [],\n",
    "                    \"eval_loss\" : []\n",
    "                   }\n",
    "    for config in configurations:\n",
    "        eval_file = fr\"outputs/{model_type}/{model_type}_\" + \\\n",
    "                    str(config[\"learning_rate\"]) + \"_\" + \\\n",
    "                    str(config[\"warmup_ratio\"]) + \"_e\" + \\\n",
    "                    str(config[\"num_train_epochs\"]) + \"_b\" + \\\n",
    "                    str(config[\"train_batch_size\"]) +\"/eval_results.txt\"\n",
    "        with open(eval_file, \"r+\") as file:\n",
    "            data = file.readlines()\n",
    "            results_dict[\"model_name\"].append(f\"{model_type}\")\n",
    "            results_dict[\"learning_rate\"].append(config[\"learning_rate\"])\n",
    "            results_dict[\"warmup_ratio\"].append(config[\"warmup_ratio\"])\n",
    "            results_dict[\"num_epochs\"].append(config[\"num_train_epochs\"])\n",
    "            results_dict[\"batch_size\"].append(config[\"train_batch_size\"])\n",
    "            results_dict[\"acc\"].append(data[0][6:-1])\n",
    "            results_dict[\"eval_loss\"].append(data[1][12:-1])\n",
    "    df = pd.DataFrame.from_dict(results_dict)\n",
    "    df.to_csv(f\"{model_type}_validation_results.csv\")\n",
    "    df_style = df.style.highlight_max(subset=\"acc\", color=\"aquamarine\", axis=0)\n",
    "    display(df_style)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7369c4",
   "metadata": {},
   "source": [
    "#### Training and Evaluation Pipeline function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eed0de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_pipeline(train_data, \n",
    "                        validation_data,\n",
    "                        columns,\n",
    "                        model_type,\n",
    "                        model_name,\n",
    "                        num_labels=3,\n",
    "                        epochs=[10],\n",
    "                        batch_size=[8],\n",
    "                        one_class=\"\",\n",
    "                        learn_rate = [4e-6, 1e-5, 4e-5, 1e-4],\n",
    "                        warmup = [0.02, 0.06, 0.1],\n",
    "                        ):\n",
    "    '''\n",
    "    Training and evaluation Pipeline:\n",
    "    \n",
    "    Args:\n",
    "        - train_data: Path to training data file (Accept .jsonl file)\n",
    "        - validation_data: Path to valdation data file (Accept .jsonl file)\n",
    "        - columns: Select relevant columns (features) used to train models (\"postText\", \"targetTitle\" or both)\n",
    "        - model_type: Select model type for simpletransformers library (\"bert\", \"distilbert\", \"deberta\")\n",
    "        - model_url: model name from Hugging Face (must be of the same type as model_type). For example:\n",
    "            + model_type: \"bert\" - model_name:\"bert-base-cased\"\n",
    "            + model_type: \"distillbert\" - model_name:\"distilbert-base-cased\"\n",
    "            + model_type: \"deberta\" - model_name:\"microsoft/deberta-base\"\n",
    "        - epochs: Number of epochs to train.\n",
    "        - batch_size: Batch size\n",
    "        - learn_rate: A list of learning rate to train the models on\n",
    "        - warmup: A list of warm up ratio to train the models on\n",
    "        \n",
    "    Returns :\n",
    "        - dataframe: DataFrame containing the results.\n",
    "    '''\n",
    "    if one_class:\n",
    "        num_labels = 2\n",
    "    train_set = load_data(train_data, columns, one_class)\n",
    "    validation_set = load_data(validation_data, columns, one_class)\n",
    "    \n",
    "    configurations = make_config(model_type=model_type, learn_rate=learn_rate, warmup=warmup, epochs=epochs, batch_size=batch_size)\n",
    "    results = train_model(train_set, validation_set, model_type, model_name, configurations, num_labels)\n",
    "    dataframe = result_table(model_type, configurations)\n",
    "    return dataframe\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
