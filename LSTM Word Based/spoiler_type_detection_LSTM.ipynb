{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Author: Stanley Joel Gona\n",
        "Last modified: 29-03-2023\n",
        "Description: An LSTM-based neural network for classifying spoiler types in clickbait posts.\n",
        "Dataset: https://pan.webis.de/semeval23/pan23-web/clickbait-challenge.html#data\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "N41hU2UytKjU",
        "outputId": "e2c3319d-027f-4980-c948-82501c1d5c04"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAuthor: Stanley Joel Gona\\nLast modified: 29-03-2023\\nDescription: An LSTM-based neural network for classifying spoiler types in clickbait posts.\\nDataset: https://pan.webis.de/semeval23/pan23-web/clickbait-challenge.html#data\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mDTq8QiGa-7",
        "outputId": "a55c26cb-18be-4edd-a372-ec305be39d6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt') # Downloading the 'punkt' package from NLTK\n",
        "nltk.download('stopwords') # Downloading the 'stopwords' package from NLTK\n",
        "\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import torch\n",
        "\n",
        "def load_jsonl(file_path, field):\n",
        "    \"\"\"\n",
        "    This function loads a JSON Lines file and returns a list of the values in the specified field.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "# Load training and validation data\n",
        "train_data = load_jsonl(\"/content/train.jsonl\", ['postText', 'targetTitle', 'tags'])\n",
        "valid_data = load_jsonl(\"/content/validation.jsonl\", ['postText', 'targetTitle', 'tags'])\n",
        "\n",
        "# Count the frequency of each word in the training data\n",
        "word_counter = Counter()\n",
        "for data_entry in training_data:\n",
        "    if not data_entry['postText'] or not data_entry['targetTitle']:\n",
        "        continue\n",
        "    text_content = data_entry['postText'][0].lower() + ' ' + data_entry['targetTitle'][0].lower()\n",
        "    text_content = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text_content)\n",
        "    text_content = re.sub(r'\\d+', '', text_content)\n",
        "    token_list = word_tokenize(text_content)\n",
        "    stop_word_set = set(stopwords.words('english'))\n",
        "    token_list = [token for token in token_list if token not in stop_word_set]\n",
        "    word_counter.update(token_list)\n",
        "\n",
        "# Create the vocabulary dictionary\n",
        "vocab_map = {'<PAD>': 0, '<UNK>': 1}\n",
        "for word, count in word_counter.most_common():\n",
        "    vocab_map[word] = len(vocab_map)\n",
        "\n",
        "def clean_dataset(dataset, vocab_map):\n",
        "    \"\"\"\n",
        "    Cleans the input dataset by performing the following operations:\n",
        "    converts all text to lowercase\n",
        "    removes punctuations and digits\n",
        "    tokenizes the text\n",
        "    removes stop words\n",
        "    maps each token to its corresponding index in the provided vocabulary dictionary\n",
        "    \"\"\"\n",
        "    cleaned_data = []\n",
        "    for data_entry in dataset:\n",
        "        if not data_entry['postText'] or not data_entry['targetTitle']:\n",
        "            continue\n",
        "        cleaned_entry = data_entry.copy()\n",
        "        content = data_entry['postText'][0].lower() + ' ' + data_entry['targetTitle'][0].lower()  # converts all text to lowercase\n",
        "        content = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", content)  # Remove punctuations\n",
        "        content = re.sub(r'\\d+', '', content)  # Removes digit\n",
        "        tokens = word_tokenize(content)  # Tokenize the text\n",
        "        stop_words = set(stopwords.words('english'))  # Remove stop words\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "        cleaned_entry['text'] = [vocab_map.get(token, 0) for token in tokens]  # 0 is the index for unknown tokens\n",
        "        cleaned_data.append(cleaned_entry)  # Map each token to its corresponding index in the provided vocabulary dictionary\n",
        "    return cleaned_data  # A list of dictionaries representing the cleaned dataset\n",
        "\n",
        "# Clean the training and validation datasets\n",
        "\n",
        "train_cleaned = clean_dataset(train_data, vocab_map)\n",
        "val_cleaned = clean_dataset(valid_data, vocab_map)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def encode_tags(tags, tag_dict):\n",
        "    \"\"\"\n",
        "    This function encodes the given list of tags using the provided tag dictionary.\n",
        "    \"\"\"\n",
        "    return [tag_dict[tag] for tag in tags] \n",
        "\n",
        "\n",
        "# Extracts the text sequences from the preprocessed training and validation data\n",
        "train_sequences = [data_entry['text'] for data_entry in train_preprocessed]\n",
        "valid_sequences = [data_entry['text'] for data_entry in valid_preprocessed]\n",
        "\n",
        "tag_dict = {'multi': 0, 'phrase': 1, 'passage': 2} # A dictionary that maps tag names to integer indices.\n",
        "\n",
        "train_encoded_tags = [encode_tags(data_entry['tags'], tag_dict) for data_entry in train_preprocessed]\n",
        "valid_encoded_tags = [encode_tags(data_entry['tags'], tag_dict) for data_entry in valid_preprocessed]\n",
        "\n",
        "\n",
        "class TextTaggingDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Defines a PyTorch Dataset class for text tagging tasks, where each data entry consists of a sequence of tokenized words and a corresponding tag.\n",
        "    \"\"\"\n",
        "    def __init__(self, sequences, tags):\n",
        "        self.sequences = sequences\n",
        "        self.tags = tags\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sequence = torch.LongTensor(self.sequences[idx])\n",
        "        tag = torch.LongTensor(self.tags[idx])\n",
        "        return sequence, tag\n",
        "    \n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    This function collates a batch of sequences and tags into padded sequences and tags.\n",
        "    \"\"\"\n",
        "    sequences = [item[0] for item in batch]\n",
        "    tags = [item[1] for item in batch]\n",
        "    sequences_padded = pad_sequence(sequences, batch_first=True)\n",
        "    tags_padded = torch.stack(tags, dim=0)\n",
        "    return sequences_padded, tags_padded\n",
        "\n",
        "# Creating instances of the TextTaggingDataset class for the preprocessed training and validation datasets  \n",
        "train_dataset = TextTaggingDataset(train_sequences, train_encoded_tags)\n",
        "valid_dataset = TextTaggingDataset(valid_sequences, valid_encoded_tags)\n",
        "\n",
        "batch_size = 64  # Choose an appropriate batch size\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "0JkHCr3PH5uZ"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_vocabulary(tokenized_texts): \n",
        "    \"\"\"\n",
        "    This function creates a vocabulary set from tokenized texts, including the token '<UNK>' for unknown words\n",
        "    \"\"\"\n",
        "    vocab = set()\n",
        "    for text in tokenized_texts:\n",
        "        vocab.update(str(word) for word in text['text'])\n",
        "    vocab.add(\"<UNK>\")\n",
        "    return sorted(list(vocab)) # returns the sorted list of vocabulary words\n",
        "\n",
        "\n",
        "def create_word_embeddings(vocab, embedding_dim):\n",
        "    \"\"\"\n",
        "    This function returns the word-to-index and index-to-word mappings, as well as the embeddings for the vocabulary\n",
        "    \"\"\"\n",
        "    word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "    index_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
        "    embeddings = np.random.uniform(-1, 1, (len(vocab), embedding_dim))\n",
        "    return word_to_index, index_to_word, embeddings\n",
        "\n",
        "# Creates a list of tokenized texts for the training data.\n",
        "train_tokenized_texts = [entry['text'] for entry in train_preprocessed]\n",
        "\n",
        "# Create vocabulary\n",
        "vocab = create_vocabulary(train_preprocessed)\n",
        "\n",
        "# Create word embeddings\n",
        "embedding_dim = 100 \n",
        "word_to_index, index_to_word, embeddings = create_word_embeddings(vocab, embedding_dim)\n",
        "\n",
        "def texts_to_sequences(tokenized_texts, word_to_index):\n",
        "    \"\"\"\n",
        "    Converts a list of tokenized texts to a list of sequences of word indices using the provided vocabulary dictionary.\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "    for text in tokenized_texts:\n",
        "        sequence = [word_to_index.get(word, word_to_index[\"<UNK>\"]) for word in text['text']]\n",
        "        sequences.append(sequence)\n",
        "    return sequences # returns a list of sequences\n",
        "\n",
        "train_sequences = texts_to_sequences(train_preprocessed, word_to_index)\n",
        "valid_sequences = texts_to_sequences(valid_preprocessed, word_to_index)\n",
        "\n",
        "\n",
        "def create_tag_mapping(tags):\n",
        "    \"\"\"\n",
        "    This function creates a mapping from unique tags to indices and vice versa for a list of tags\n",
        "    \"\"\"\n",
        "    unique_tags = []\n",
        "    for tags_entry in tags:\n",
        "        unique_tags.extend(tags_entry)\n",
        "    unique_tags = sorted(list(set(unique_tags)))\n",
        "    tag_to_index = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
        "    index_to_tag = {idx: tag for idx, tag in enumerate(unique_tags)}\n",
        "    return tag_to_index, index_to_tag\n",
        "\n",
        "tags = [entry['tags'] for entry in train_preprocessed]\n",
        "tag_to_index, index_to_tag = create_tag_mapping(tags)\n",
        "\n",
        "def encode_tags(tag_data, tag_to_index):\n",
        "    \"\"\"\n",
        "    Encodes tag data using a provided tag-to-index mapping dictionary.\n",
        "    \"\"\"\n",
        "    encoded_tags = [[tag_to_index[tag] for tag in tags_entry] for tags_entry in tag_data]\n",
        "    return encoded_tags # Returns a list of lists of encoded tags\n",
        "\n",
        "train_encoded_tags = encode_tags([entry['tags'] for entry in train_preprocessed], tag_to_index)\n",
        "valid_encoded_tags = encode_tags([entry['tags'] for entry in valid_preprocessed], tag_to_index)\n"
      ],
      "metadata": {
        "id": "OK87qpyEH8MX"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class LSTMClassifier(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Defines a PyTorch module using an LSTM model\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, num_classes)\n",
        "        self.hidden_size = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        This function defines the forward pass of the LSTMClassifier model, which takes in a sequence of tokenized text as input \n",
        "        and returns the logits for each class as output.\n",
        "        \"\"\"\n",
        "        embedded = self.embedding(x)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        output, _ = self.lstm(embedded, (h0, c0))\n",
        "        last_output = output[:, -1, :]\n",
        "        logits = self.fc(last_output)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Define the model hyperparameters\n",
        "vocab_size = len(vocab_map)\n",
        "hidden_dim = 96\n",
        "num_classes = len(tag_to_index)\n",
        "\n",
        "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, num_classes)\n",
        "\n",
        "# loss function and optimizer\n",
        "learning_rate = 0.007\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 25\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    for sequences, tags in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(sequences)\n",
        "        loss = criterion(logits, tags[:, 0])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * sequences.size(0)\n",
        "        train_correct += (torch.argmax(logits, dim=1) == tags[:, 0]).sum().item()\n",
        "    train_loss /= len(train_dataset)\n",
        "    train_acc = train_correct / len(train_dataset)\n",
        "    \n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    valid_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for sequences, tags in valid_loader:\n",
        "            logits = model(sequences)\n",
        "            loss = criterion(logits, tags[:, 0])\n",
        "            valid_loss += loss.item() * sequences.size(0)\n",
        "            valid_correct += (torch.argmax(logits, dim=1) == tags[:, 0]).sum().item()\n",
        "    valid_loss /= len(valid_dataset)\n",
        "    valid_acc = valid_correct / len(valid_dataset)\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, Val Loss={valid_loss:.4f}, Val Acc={valid_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0i4l0R3H_EX",
        "outputId": "0110bd9b-79cf-4017-911e-04da70fdbb72"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss=1.0475, Train Acc=0.4242, Val Loss=1.0409, Val Acc=0.4293\n",
            "Epoch 2: Train Loss=1.0329, Train Acc=0.4317, Val Loss=1.0459, Val Acc=0.3955\n",
            "Epoch 3: Train Loss=1.0201, Train Acc=0.4273, Val Loss=1.0153, Val Acc=0.4593\n",
            "Epoch 4: Train Loss=0.9046, Train Acc=0.5536, Val Loss=0.9996, Val Acc=0.5194\n",
            "Epoch 5: Train Loss=0.5802, Train Acc=0.7765, Val Loss=1.1398, Val Acc=0.5369\n",
            "Epoch 6: Train Loss=0.2973, Train Acc=0.9047, Val Loss=1.2911, Val Acc=0.5432\n",
            "Epoch 7: Train Loss=0.1335, Train Acc=0.9609, Val Loss=1.6751, Val Acc=0.5219\n",
            "Epoch 8: Train Loss=0.0559, Train Acc=0.9841, Val Loss=2.1053, Val Acc=0.5344\n",
            "Epoch 9: Train Loss=0.0305, Train Acc=0.9906, Val Loss=2.0070, Val Acc=0.5357\n",
            "Epoch 10: Train Loss=0.0243, Train Acc=0.9925, Val Loss=2.1248, Val Acc=0.5357\n",
            "Epoch 11: Train Loss=0.0236, Train Acc=0.9919, Val Loss=2.3595, Val Acc=0.5244\n",
            "Epoch 12: Train Loss=0.0116, Train Acc=0.9962, Val Loss=2.5999, Val Acc=0.5369\n",
            "Epoch 13: Train Loss=0.0090, Train Acc=0.9975, Val Loss=2.6595, Val Acc=0.5307\n",
            "Epoch 14: Train Loss=0.0115, Train Acc=0.9966, Val Loss=2.5335, Val Acc=0.5407\n",
            "Epoch 15: Train Loss=0.0281, Train Acc=0.9909, Val Loss=2.7172, Val Acc=0.5257\n",
            "Epoch 16: Train Loss=0.0319, Train Acc=0.9897, Val Loss=2.2590, Val Acc=0.5194\n",
            "Epoch 17: Train Loss=0.0139, Train Acc=0.9956, Val Loss=2.6696, Val Acc=0.5144\n",
            "Epoch 18: Train Loss=0.0074, Train Acc=0.9975, Val Loss=2.9077, Val Acc=0.5194\n",
            "Epoch 19: Train Loss=0.0069, Train Acc=0.9969, Val Loss=2.9207, Val Acc=0.5269\n",
            "Epoch 20: Train Loss=0.0039, Train Acc=0.9981, Val Loss=3.1221, Val Acc=0.5269\n",
            "Epoch 21: Train Loss=0.0028, Train Acc=0.9987, Val Loss=3.0984, Val Acc=0.5257\n",
            "Epoch 22: Train Loss=0.0025, Train Acc=0.9987, Val Loss=3.1502, Val Acc=0.5307\n",
            "Epoch 23: Train Loss=0.0022, Train Acc=0.9987, Val Loss=3.2470, Val Acc=0.5244\n",
            "Epoch 24: Train Loss=0.0021, Train Acc=0.9981, Val Loss=3.2582, Val Acc=0.5257\n",
            "Epoch 25: Train Loss=0.0021, Train Acc=0.9984, Val Loss=3.3240, Val Acc=0.5344\n"
          ]
        }
      ]
    }
  ]
}