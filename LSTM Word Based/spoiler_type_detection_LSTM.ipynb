{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Author: Stanley Joel Gona\n",
        "Last modified: 31-03-2023\n",
        "Description: An LSTM-based neural network for classifying spoiler types in clickbait posts.\n",
        "Dataset: https://pan.webis.de/semeval23/pan23-web/clickbait-challenge.html#data\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "N41hU2UytKjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mDTq8QiGa-7",
        "outputId": "93ed7e71-1c68-44cb-eb2d-c5a47a59f61c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt') # Downloading the 'punkt' package from NLTK\n",
        "nltk.download('stopwords') # Downloading the 'stopwords' package from NLTK\n",
        "\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import torch\n",
        "\n",
        "def load_jsonl(file_path, field):\n",
        "    \"\"\"\n",
        "    Load values from a specific field in a JSON Lines file.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to the JSON Lines file.\n",
        "        field: Field to extract values from.\n",
        "\n",
        "    Returns:\n",
        "        List of extracted field values.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "# Load training and validation data\n",
        "train_data = load_jsonl(\"/content/train.jsonl\", ['postText', 'targetTitle', 'tags'])\n",
        "valid_data = load_jsonl(\"/content/validation.jsonl\", ['postText', 'targetTitle', 'tags'])\n",
        "\n",
        "# Count the frequency of each word in the training data\n",
        "word_counter = Counter()\n",
        "for data_entry in train_data:\n",
        "    if not data_entry['postText'] or not data_entry['targetTitle']:\n",
        "        continue\n",
        "    text_content = data_entry['postText'][0].lower() + ' ' + data_entry['targetTitle'][0].lower()\n",
        "    text_content = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text_content)\n",
        "    text_content = re.sub(r'\\d+', '', text_content)\n",
        "    token_list = word_tokenize(text_content)\n",
        "    stop_word_set = set(stopwords.words('english'))\n",
        "    token_list = [token for token in token_list if token not in stop_word_set]\n",
        "    word_counter.update(token_list)\n",
        "\n",
        "# Create the vocabulary dictionary\n",
        "vocab_map = {'<PAD>': 0, '<UNK>': 1}\n",
        "for word, count in word_counter.most_common():\n",
        "    vocab_map[word] = len(vocab_map)\n",
        "\n",
        "def clean_dataset(dataset, vocab_map):\n",
        "    \"\"\"\n",
        "    Cleans the input dataset by performing the following operations:\n",
        "    converts all text to lowercase\n",
        "    removes punctuations and digits\n",
        "    tokenizes the text\n",
        "    removes stop words\n",
        "    maps each token to its corresponding index in the provided vocabulary dictionary\n",
        "\n",
        "\n",
        "    Args:\n",
        "        dataset: List of dictionaries containing 'postText' and 'targetTitle' keys.\n",
        "        vocab_map: Dictionary mapping tokens to their corresponding indices.\n",
        "\n",
        "    Returns:\n",
        "        List of cleaned dictionaries.\n",
        "    \"\"\"\n",
        "    cleaned_data = []\n",
        "    for data_entry in dataset:\n",
        "        if not data_entry['postText'] or not data_entry['targetTitle']:\n",
        "            continue\n",
        "        cleaned_entry = data_entry.copy()\n",
        "        content = data_entry['postText'][0].lower() + ' ' + data_entry['targetTitle'][0].lower()  # converts all text to lowercase\n",
        "        content = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", content)  # Remove punctuations\n",
        "        content = re.sub(r'\\d+', '', content)  # Removes digit\n",
        "        tokens = word_tokenize(content)  # Tokenize the text\n",
        "        stop_words = set(stopwords.words('english'))  # Remove stop words\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "        cleaned_entry['text'] = [vocab_map.get(token, 0) for token in tokens]  # 0 is the index for unknown tokens\n",
        "        cleaned_data.append(cleaned_entry)  # Map each token to its corresponding index in the provided vocabulary dictionary\n",
        "    return cleaned_data  # A list of dictionaries representing the cleaned dataset\n",
        "\n",
        "# Clean the training and validation datasets\n",
        "\n",
        "train_preprocessed = clean_dataset(train_data, vocab_map)\n",
        "valid_preprocessed = clean_dataset(valid_data, vocab_map)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def encode_tags(tags, tag_dict):\n",
        "    \"\"\"\n",
        "    This function encodes the given list of tags using the provided tag dictionary\n",
        "\n",
        "    Args:\n",
        "        tags (list): A list of tags to be encoded.\n",
        "        tag_dict (dict): A dictionary mapping tags to their corresponding indices\n",
        "\n",
        "    Returns:\n",
        "        list: A list of encoded tags as integers.\n",
        "    \"\"\"\n",
        "    return [tag_dict[tag] for tag in tags] \n",
        "\n",
        "\n",
        "# Extracts the text sequences from the preprocessed training and validation data\n",
        "train_sequences = [data_entry['text'] for data_entry in train_preprocessed]\n",
        "valid_sequences = [data_entry['text'] for data_entry in valid_preprocessed]\n",
        "\n",
        "tag_dict = {'multi': 0, 'phrase': 1, 'passage': 2} # A dictionary that maps tag names to integer indices.\n",
        "\n",
        "train_encoded_tags = [encode_tags(data_entry['tags'], tag_dict) for data_entry in train_preprocessed]\n",
        "valid_encoded_tags = [encode_tags(data_entry['tags'], tag_dict) for data_entry in valid_preprocessed]\n",
        "\n",
        "\n",
        "class TextTaggingDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Defines a PyTorch Dataset class for text tagging tasks, where each data entry consists of a sequence of tokenized words and a corresponding tag.\n",
        "    \"\"\"\n",
        "    def __init__(self, sequences, tags):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          sequences (list): A list of tokenized word sequences\n",
        "          tags (list): A list of corresponding tag sequences \n",
        "        \"\"\"\n",
        "        self.sequences = sequences\n",
        "        self.tags = tags\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sequence = torch.LongTensor(self.sequences[idx])\n",
        "        tag = torch.LongTensor(self.tags[idx])\n",
        "        return sequence, tag\n",
        "    \n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    This function collates a batch of sequences and tags into padded sequences and tags.\n",
        "\n",
        "    Args:\n",
        "        batch (list): A list of tuples containing a sequence tensor and the corresponding tag tensor\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the padded sequences tensor and the padded tags tensor\n",
        "    \"\"\"\n",
        "    sequences = [item[0] for item in batch]\n",
        "    tags = [item[1] for item in batch]\n",
        "    sequences_padded = pad_sequence(sequences, batch_first=True)\n",
        "    tags_padded = torch.stack(tags, dim=0)\n",
        "    return sequences_padded, tags_padded\n",
        "\n",
        "# Creating instances of the TextTaggingDataset class for the preprocessed training and validation datasets  \n",
        "train_dataset = TextTaggingDataset(train_sequences, train_encoded_tags)\n",
        "valid_dataset = TextTaggingDataset(valid_sequences, valid_encoded_tags)\n",
        "\n",
        "batch_size = 64  # Choose an appropriate batch size\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "0JkHCr3PH5uZ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_vocabulary(tokenized_texts): \n",
        "    \"\"\"\n",
        "    This function creates a vocabulary set from tokenized texts, including the token '<UNK>' for unknown words\n",
        "\n",
        "    Args:\n",
        "        A list of tokenized text\n",
        "\n",
        "    Returns:\n",
        "        Returns the sorted list of vocabulary words\n",
        "\n",
        "    \"\"\"\n",
        "    vocab = set()\n",
        "    for text in tokenized_texts:\n",
        "        vocab.update(str(word) for word in text['text'])\n",
        "    vocab.add(\"<UNK>\")\n",
        "    return sorted(list(vocab)) \n",
        "\n",
        "\n",
        "def create_word_embeddings(vocab, embedding_dim):\n",
        "    \"\"\"\n",
        "    This function returns the word-to-index and index-to-word mappings, as well as the embeddings for the vocabulary\n",
        "\n",
        "    Args:\n",
        "        vocab (list): A list of unique vocabulary words.\n",
        "        embedding_dim (int): The dimension of the word embeddings.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the word-to-index dictionary, the index-to-word dictionary, and the embeddings array.\n",
        "    \"\"\"\n",
        "    word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "    index_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
        "    embeddings = np.random.uniform(-1, 1, (len(vocab), embedding_dim))\n",
        "    return word_to_index, index_to_word, embeddings\n",
        "\n",
        "# Creates a list of tokenized texts for the training data.\n",
        "train_tokenized_texts = [entry['text'] for entry in train_preprocessed]\n",
        "\n",
        "# Create vocabulary\n",
        "vocab = create_vocabulary(train_preprocessed)\n",
        "\n",
        "# Create word embeddings\n",
        "embedding_dim = 100 \n",
        "word_to_index, index_to_word, embeddings = create_word_embeddings(vocab, embedding_dim)\n",
        "\n",
        "def texts_to_sequences(tokenized_texts, word_to_index):\n",
        "    \"\"\"\n",
        "    Converts a list of tokenized texts to a list of sequences of word indices using the provided vocabulary dictionary.\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "    for text in tokenized_texts:\n",
        "        sequence = [word_to_index.get(word, word_to_index[\"<UNK>\"]) for word in text['text']]\n",
        "        sequences.append(sequence)\n",
        "    return sequences # returns a list of sequences\n",
        "\n",
        "train_sequences = texts_to_sequences(train_preprocessed, word_to_index)\n",
        "valid_sequences = texts_to_sequences(valid_preprocessed, word_to_index)\n",
        "\n",
        "\n",
        "def create_tag_mapping(tags):\n",
        "    \"\"\"\n",
        "    This function creates a mapping from unique tags to indices and vice versa for a list of tags\n",
        "    \"\"\"\n",
        "    unique_tags = []\n",
        "    for tags_entry in tags:\n",
        "        unique_tags.extend(tags_entry)\n",
        "    unique_tags = sorted(list(set(unique_tags)))\n",
        "    tag_to_index = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
        "    index_to_tag = {idx: tag for idx, tag in enumerate(unique_tags)}\n",
        "    return tag_to_index, index_to_tag\n",
        "\n",
        "tags = [entry['tags'] for entry in train_preprocessed]\n",
        "tag_to_index, index_to_tag = create_tag_mapping(tags)\n",
        "\n",
        "def encode_tags(tag_data, tag_to_index):\n",
        "    \"\"\"\n",
        "    Encodes tag data using a provided tag-to-index mapping dictionary.\n",
        "    \"\"\"\n",
        "    encoded_tags = [[tag_to_index[tag] for tag in tags_entry] for tags_entry in tag_data]\n",
        "    return encoded_tags # Returns a list of lists of encoded tags\n",
        "\n",
        "train_encoded_tags = encode_tags([entry['tags'] for entry in train_preprocessed], tag_to_index)\n",
        "valid_encoded_tags = encode_tags([entry['tags'] for entry in valid_preprocessed], tag_to_index)\n"
      ],
      "metadata": {
        "id": "OK87qpyEH8MX"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class LSTMClassifier(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Defines a PyTorch module using an LSTM model\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, num_classes)\n",
        "        self.hidden_size = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        This function defines the forward pass of the LSTMClassifier model, which takes in a sequence of tokenized text as input \n",
        "        and returns the logits for each class as output.\n",
        "        \"\"\"\n",
        "        embedded = self.embedding(x)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        output, _ = self.lstm(embedded, (h0, c0))\n",
        "        last_output = output[:, -1, :]\n",
        "        logits = self.fc(last_output)\n",
        "        return logits\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a0i4l0R3H_EX"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab_map)\n",
        "hidden_dims = [64,96, 128, 256, 512]\n",
        "learning_rates = [0.0004, 0.0005, 0.001, 0.002, 0.004 , 0.005]\n",
        "num_classes = len(tag_to_index)\n",
        "\n",
        "# Print header\n",
        "header = \"{:<10} {:<15} {:<15} {:<15} {:<15} {:<15}\".format(\"Epoch\", \"Hidden dim\", \"Learning rate\", \"Train Loss\", \"Train Acc\", \"Val Acc\")\n",
        "print(header)\n",
        "print(\"-\" * len(header))\n",
        "\n",
        "for hidden_dim in hidden_dims:\n",
        "    for learning_rate in learning_rates:\n",
        "\n",
        "        # Define the model with the current hidden_dim\n",
        "        model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, num_classes)\n",
        "\n",
        "        # Loss function and optimizer\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "        \n",
        "\n",
        "        # Train the model\n",
        "        num_epochs = 25\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            train_correct = 0\n",
        "            for sequences, tags in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(sequences)\n",
        "                loss = criterion(logits, tags[:, 0])\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item() * sequences.size(0)\n",
        "                train_correct += (torch.argmax(logits, dim=1) == tags[:, 0]).sum().item()\n",
        "            train_loss /= len(train_dataset)\n",
        "            train_acc = train_correct / len(train_dataset)\n",
        "\n",
        "            model.eval()\n",
        "            valid_correct = 0\n",
        "            with torch.no_grad():\n",
        "                valid_loss = 0.0\n",
        "                for sequences, tags in valid_loader:\n",
        "                    logits = model(sequences)\n",
        "                    loss = criterion(logits, tags[:, 0])\n",
        "                    valid_loss += loss.item() * sequences.size(0)\n",
        "                    valid_correct += (torch.argmax(logits, dim=1) == tags[:, 0]).sum().item()\n",
        "                valid_loss /= len(valid_dataset)\n",
        "                valid_acc = valid_correct / len(valid_dataset)\n",
        "\n",
        "            if epoch == 24:\n",
        "                row = \"{:<10} {:<15} {:<15.7f} {:<15.4f} {:<15.4f} {:<15.4f}\".format(epoch + 1, hidden_dim, learning_rate, train_loss, train_acc, valid_acc)\n",
        "                print(row)\n",
        "\n",
        "        print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-CHmjYOOqku",
        "outputId": "e8efe753-a8ce-4666-c77e-2410808466f7"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch      Hidden dim      Learning rate   Train Loss      Train Acc       Val Acc        \n",
            "------------------------------------------------------------------------------------------\n",
            "25         64              0.0004000       0.1125          0.9694          0.4406         \n",
            "\n",
            "\n",
            "25         64              0.0005000       0.0940          0.9778          0.4956         \n",
            "\n",
            "\n",
            "25         64              0.0010000       0.0174          0.9969          0.4393         \n",
            "\n",
            "\n",
            "25         64              0.0020000       0.0024          0.9987          0.5357         \n",
            "\n",
            "\n",
            "25         64              0.0040000       0.0019          0.9981          0.5232         \n",
            "\n",
            "\n",
            "25         64              0.0050000       0.0020          0.9987          0.5181         \n",
            "\n",
            "\n",
            "25         96              0.0004000       0.0942          0.9794          0.4568         \n",
            "\n",
            "\n",
            "25         96              0.0005000       0.0574          0.9862          0.4781         \n",
            "\n",
            "\n",
            "25         96              0.0010000       0.0110          0.9975          0.4994         \n",
            "\n",
            "\n",
            "25         96              0.0020000       0.0103          0.9969          0.5019         \n",
            "\n",
            "\n",
            "25         96              0.0040000       0.0035          0.9984          0.4806         \n",
            "\n",
            "\n",
            "25         96              0.0050000       0.0023          0.9984          0.5344         \n",
            "\n",
            "\n",
            "25         128             0.0004000       0.1007          0.9750          0.4869         \n",
            "\n",
            "\n",
            "25         128             0.0005000       0.0304          0.9953          0.5106         \n",
            "\n",
            "\n",
            "25         128             0.0010000       0.0128          0.9969          0.5069         \n",
            "\n",
            "\n",
            "25         128             0.0020000       0.0036          0.9981          0.4831         \n",
            "\n",
            "\n",
            "25         128             0.0040000       0.0018          0.9984          0.5219         \n",
            "\n",
            "\n",
            "25         128             0.0050000       0.0026          0.9984          0.4956         \n",
            "\n",
            "\n",
            "25         256             0.0004000       0.0305          0.9937          0.5282         \n",
            "\n",
            "\n",
            "25         256             0.0005000       0.0213          0.9947          0.5081         \n",
            "\n",
            "\n",
            "25         256             0.0010000       0.0025          0.9987          0.5019         \n",
            "\n",
            "\n",
            "25         256             0.0020000       0.0018          0.9987          0.5144         \n",
            "\n",
            "\n",
            "25         256             0.0040000       0.0018          0.9987          0.4869         \n",
            "\n",
            "\n",
            "25         256             0.0050000       0.0019          0.9987          0.5232         \n",
            "\n",
            "\n",
            "25         512             0.0004000       0.0455          0.9847          0.4355         \n",
            "\n",
            "\n",
            "25         512             0.0005000       0.0070          0.9984          0.5169         \n",
            "\n",
            "\n",
            "25         512             0.0010000       0.0026          0.9991          0.4944         \n",
            "\n",
            "\n",
            "25         512             0.0020000       0.0021          0.9984          0.5056         \n",
            "\n",
            "\n",
            "25         512             0.0040000       0.0022          0.9981          0.5094         \n",
            "\n",
            "\n",
            "25         512             0.0050000       0.0020          0.9984          0.5257         \n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}